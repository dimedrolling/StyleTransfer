{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a class for applying random style deformations to images\n",
    "class RandomStyleDeformation:\n",
    "    def __init__(self, deformation_list):\n",
    "        self.deformation_list = deformation_list\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # Randomly select a style deformation function from the list\n",
    "        deformation_func = random.choice(self.deformation_list)\n",
    "        \n",
    "        # Apply the selected deformation to the image\n",
    "        return deformation_func(image)\n",
    "\n",
    "# Define a random rotation style deformation function\n",
    "def style_deformation_1(image):\n",
    "    # Apply a random color jitter transformation\n",
    "    color_jitter = transforms.ColorJitter(\n",
    "        brightness=random.uniform(0.8, 1.2),\n",
    "        contrast=random.uniform(0.8, 1.2),\n",
    "        saturation=random.uniform(0.8, 1.2),\n",
    "        hue=random.uniform(0.1, 0.2)\n",
    "    )\n",
    "    image = color_jitter(image)\n",
    "    return image\n",
    "\n",
    "# Define a random brightness adjustment style deformation function\n",
    "def style_deformation_2(image):\n",
    "    # Apply a random brightness adjustment\n",
    "    brightness_factor = random.uniform(0.8, 1.2)\n",
    "    image = transforms.functional.adjust_brightness(image, brightness_factor)\n",
    "    return image\n",
    "\n",
    "# List of style deformation functions\n",
    "deformation_list = [style_deformation_1, style_deformation_2]\n",
    "\n",
    "# Create a RandomStyleDeformation object\n",
    "random_style_deformation = RandomStyleDeformation(deformation_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the model architecture (e.g., 'efficientnet_b0' for the smallest EfficientNet)\n",
    "model_name = 'efficientnet_b0'\n",
    "\n",
    "# Load the pre-trained EfficientNet-B0 model from a local file\n",
    "model_file = model_name + '.pth'\n",
    "efficientnet_b0 = torch.load(model_file).eval()\n",
    "\n",
    "# Set the number of classes in your task (adjust as needed)\n",
    "k = 16\n",
    "out_features = k * k\n",
    "\n",
    "# Replace the final classifier with a new fully connected layer\n",
    "in_features = efficientnet_b0.classifier.in_features\n",
    "# Replace the final classifier with two branches, each outputting a vector\n",
    "efficientnet_b0.classifier = nn.Linear(in_features, 2 * out_features)  # 2 stands for two vectors, r and d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define image transformations for preprocessing\n",
    "downsample_size = (224, 224)  # Desired downsampled size\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(downsample_size),  # Downsample the image to match model input size\n",
    "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "])\n",
    "\n",
    "# Define a custom dataset for your test images\n",
    "class TestImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Path to your test images\n",
    "image_paths = ['data/1674921468776855 (2).jpeg']\n",
    "\n",
    "# Define the height and width for resizing\n",
    "height, width = 1536, 1536\n",
    "\n",
    "# Define image transformations (you can adjust these as needed)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((height, width)),  # Resize your images to the desired size\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    # Add more transformations as necessary, e.g., normalization\n",
    "])\n",
    "\n",
    "# Create a DataLoader for your test images\n",
    "test_dataset = TestImageDataset(image_paths, transform=image_transform)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Batch size 1 to process one image at a time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the MatrixProductModel\n",
    "class MatrixProductModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MatrixProductModel, self).__init__()\n",
    "        \n",
    "        self.R = nn.Parameter(torch.randn(input_dim, output_dim), requires_grad=True)\n",
    "        self.Q = nn.Parameter(torch.randn(output_dim, input_dim), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, T):\n",
    "        # Perform matrix-vector multiplication Q^T * T * R * x\n",
    "        y =  torch.matmul(torch.matmul(torch.matmul(x, self.R), T), self.Q)\n",
    "        return y\n",
    "\n",
    "# Custom loss function that combines L1 norms and L2 norm\n",
    "def custom_loss(Z_1, Z_2, Y_1, Y_2, I_1, I_2, coef_l):\n",
    "    # Calculate L2 norm\n",
    "    l2_norm = F.mse_loss(Z_1, Z_2)\n",
    "\n",
    "    # Calculate L1 norms\n",
    "    l1_norm1 = F.l1_loss(Y_1, I_1)\n",
    "    l1_norm2 = F.l1_loss(Y_2, I_2)\n",
    "\n",
    "    # Combine the L1 norms as needed\n",
    "    loss = coef_l * l2_norm + l1_norm1 + l1_norm2\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lambda coefficient\n",
    "coef_l = 10\n",
    "\n",
    "# Initialize the model and optimizer with Adam\n",
    "model_n = MatrixProductModel(3, k)\n",
    "model_s = MatrixProductModel(3, k)\n",
    "\n",
    "optimizer_n = optim.Adam(model_n.parameters(), lr=0.01)\n",
    "optimizer_s = optim.Adam(model_s.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7440ac92436140b6898f29cff1c7fa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 30.6618\n",
      "Epoch 1: Loss = 1.7765\n",
      "Epoch 2: Loss = 1.8757\n",
      "Epoch 3: Loss = 9.6255\n",
      "Epoch 4: Loss = 7.5265\n",
      "Epoch 5: Loss = 17.8318\n",
      "Epoch 6: Loss = 3.7520\n",
      "Epoch 7: Loss = 19.9365\n",
      "Epoch 8: Loss = 7.7490\n",
      "Epoch 9: Loss = 0.9695\n",
      "Epoch 10: Loss = 5.6036\n",
      "Epoch 11: Loss = 4.2850\n",
      "Epoch 12: Loss = 2.2225\n",
      "Epoch 13: Loss = 1.4507\n",
      "Epoch 14: Loss = 3.4479\n",
      "Epoch 15: Loss = 1.1582\n",
      "Epoch 16: Loss = 1.4837\n",
      "Epoch 17: Loss = 0.6115\n",
      "Epoch 18: Loss = 0.4796\n",
      "Epoch 19: Loss = 4.2852\n",
      "Epoch 20: Loss = 0.6729\n",
      "Epoch 21: Loss = 1.5469\n",
      "Epoch 22: Loss = 0.5737\n",
      "Epoch 23: Loss = 3.1638\n",
      "Epoch 24: Loss = 0.2474\n",
      "Epoch 25: Loss = 16.0829\n",
      "Epoch 26: Loss = 4.1785\n",
      "Epoch 27: Loss = 2.3873\n",
      "Epoch 28: Loss = 0.1305\n",
      "Epoch 29: Loss = 0.0660\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\DSprojects\\Diffusion\\test.ipynb Cell 7\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/DSprojects/Diffusion/test.ipynb#Y102sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Calculate loss, coef lambda = 10\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/DSprojects/Diffusion/test.ipynb#Y102sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m custom_loss(Z_1, Z_2, Y_1, Y_2, x1\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m), x2\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m), coef_l)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projects/DSprojects/Diffusion/test.ipynb#Y102sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/DSprojects/Diffusion/test.ipynb#Y102sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m optimizer_n\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projects/DSprojects/Diffusion/test.ipynb#Y102sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m optimizer_s\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\dimav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dimav\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    efficientnet_b0.train() \n",
    "    model_n.train()\n",
    "    model_s.train()\n",
    "\n",
    "    for image in test_dataloader:\n",
    "\n",
    "        # Get random style transformations\n",
    "        x1 = random_style_deformation(image)\n",
    "        x2 = random_style_deformation(image)\n",
    "\n",
    "        # Get T from the encoder \n",
    "        downsampled_x1 = F.interpolate(x1, size=downsample_size, mode='bilinear', align_corners=False)\n",
    "        downsampled_x2 = F.interpolate(x2, size=downsample_size, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Output features = 256, so k is fixed to 16\n",
    "        # d_i, r_i - normalized color space, color style\n",
    "        d_1, r_1 = efficientnet_b0(downsampled_x1).chunk(2, dim=1)\n",
    "        d_2, r_2 = efficientnet_b0(downsampled_x1).chunk(2, dim=1)\n",
    "\n",
    "        optimizer_n.zero_grad()\n",
    "        optimizer_s.zero_grad()\n",
    "\n",
    "        # Get normalized color space pictures\n",
    "        Z_1 = model_n(x1.reshape(-1, 3), d_1.reshape(k, k))\n",
    "        Z_2 = model_n(x2.reshape(-1, 3), d_2.reshape(k, k))\n",
    "\n",
    "        # Train them to get the same color style\n",
    "        Y_1 = model_s(Z_1, r_2.reshape(k, k))\n",
    "        Y_2 = model_s(Z_2, r_1.reshape(k, k))\n",
    "\n",
    "        # Calculate loss, coef lambda = 10\n",
    "        loss = custom_loss(Z_1, Z_2, Y_1, Y_2, x1.reshape(-1, 3), x2.reshape(-1, 3), coef_l)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer_n.step()\n",
    "        optimizer_s.step()\n",
    "        \n",
    "        # Print the loss value at each step\n",
    "        print(\"Epoch {}: Loss = {:.4f}\".format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
